{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyarrow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ttest_ind\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# from bus_transformer import *\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer\n\u001b[1;32m     17\u001b[0m DEVICE \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/NLP/lib/python3.10/site-packages/datasets/__init__.py:18\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# ruff: noqa\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Copyright 2020 The HuggingFace Datasets Authors and the TensorFlow Datasets Authors.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2.19.1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_dataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_reader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ReadInstruction\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuilder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ArrowBasedBuilder, BeamBasedBuilder, BuilderConfig, DatasetBuilder, GeneratorBasedBuilder\n",
      "File \u001b[0;32m~/anaconda3/envs/NLP/lib/python3.10/site-packages/datasets/arrow_dataset.py:60\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpa\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompute\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpc\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfsspec\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m url_to_fs\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyarrow'"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random\n",
    "from torch import optim\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List\n",
    "from utils import *\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler\n",
    "import tqdm\n",
    "from scipy.stats import ttest_ind\n",
    "# from bus_transformer import *\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "seq_len = 128 \n",
    "batch_size = 16 \n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iters = 5000\n",
    "eval_interval = 100\n",
    "eval_iters = 200\n",
    "test_iters = 1000\n",
    "vocab_size = 9128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, d_model, d_internal):\n",
    "        super().__init__()\n",
    "\n",
    "        self.W_Q = torch.nn.Linear(d_model, d_internal, False)\n",
    "        self.W_K = torch.nn.Linear(d_model, d_internal, False)\n",
    "        self.W_V = torch.nn.Linear(d_model, d_internal, False)\n",
    "\n",
    "        self.SoftMax = torch.nn.Softmax(dim=-1)\n",
    "\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_internal = d_internal\n",
    "        self.norm = torch.tensor(d_model**-0.5)\n",
    "        self.tril = torch.tril(torch.ones(seq_len, seq_len, device=DEVICE))\n",
    "\n",
    "    def expand(self, d_mnew, d_inew):\n",
    "\n",
    "        self.W_Q.weight.data = torch.cat([self.W_Q.weight.data, torch.zeros(d_inew - self.d_internal, self.d_model, device=DEVICE)], dim=0)\n",
    "        self.W_Q.weight.data = torch.cat([self.W_Q.weight.data, torch.zeros(d_inew, d_mnew - self.d_model, device=DEVICE)], dim=1)\n",
    "        for i in range(self.d_internal, d_inew):\n",
    "            self.W_Q.weight.data[i][i] = self.W_Q.weight.data[i][i] if self.W_Q.weight.data[i][i] != 0 else 1\n",
    "\n",
    "        self.W_K.weight.data = torch.cat([self.W_K.weight.data, torch.zeros(d_inew - self.d_internal, self.d_model, device=DEVICE)], dim=0)\n",
    "        self.W_K.weight.data = torch.cat([self.W_K.weight.data, torch.zeros(d_inew, d_mnew - self.d_model, device=DEVICE)], dim=1)\n",
    "        for i in range(self.d_internal, d_inew):\n",
    "            self.W_K.weight.data[i][i] = self.W_K.weight.data[i][i] if self.W_K.weight.data[i][i] != 0 else 1\n",
    "\n",
    "        self.W_V.weight.data = torch.cat([self.W_V.weight.data, torch.zeros(d_inew - self.d_internal, self.d_model, device=DEVICE)], dim=0)\n",
    "        self.W_V.weight.data = torch.cat([self.W_V.weight.data, torch.zeros(d_inew, d_mnew - self.d_model, device=DEVICE)], dim=1)\n",
    "        for i in range(self.d_internal, d_inew):\n",
    "            self.W_V.weight.data[i][i] = self.W_V.weight.data[i][i] if self.W_V.weight.data[i][i] != 0 else 1\n",
    "\n",
    "        self.d_internal = d_inew\n",
    "        self.d_model = d_mnew \n",
    "        self.SoftMax = torch.nn.Softmax(dim=-1)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, input_vecs):\n",
    "        B, T, C = input_vecs.shape\n",
    "\n",
    "        Q = self.W_Q(input_vecs)\n",
    "        K = self.W_K(input_vecs)\n",
    "        V = self.W_V(input_vecs)\n",
    "\n",
    "        weights = Q @ K.transpose(-2, -1) * C**-0.5\n",
    "        weights = weights.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        Attn = self.SoftMax(weights)\n",
    "\n",
    "\n",
    "        out = Attn @ V\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_hidden):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_internal = d_model//num_heads\n",
    "        self.num_heads = num_heads\n",
    "        self.d_hidden = d_hidden\n",
    "\n",
    "        self.heads = nn.ModuleList([AttentionHead(d_model, self.d_internal) for _ in range(num_heads)])\n",
    "        self.Softmax = torch.nn.LogSoftmax(dim=-1)\n",
    "        self.FFN = torch.nn.Sequential(\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.1),\n",
    "            torch.nn.Linear(self.d_hidden, self.d_hidden),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.1),\n",
    "        )\n",
    "        self.W_O = torch.nn.Linear(d_model, d_model, False)\n",
    "        self.layernorm1 = torch.nn.LayerNorm(d_model)\n",
    "        self.layernorm2 = torch.nn.LayerNorm(d_model)\n",
    "\n",
    "        self.connection = torch.nn.Linear(d_model, self.d_hidden)\n",
    "        self.cout = torch.nn.Linear(self.d_hidden, d_model)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: input embeddings \n",
    "        :return: output of decoder block, same shape as input\n",
    "        \"\"\"\n",
    "        t = self.layernorm1(x)\n",
    "        t = torch.cat([head(t) for head in self.heads], dim=-1)\n",
    "        t = self.W_O(t)\n",
    "        t1 = self.layernorm2(t + x)\n",
    "        t = self.relu(self.cout(self.FFN(self.connection(t1))))\n",
    "        # t = self.layernorm2(t + t1)\n",
    "\n",
    "        return t\n",
    "\n",
    "\n",
    "\n",
    "    def expand(self, d_mnew, d_inew):\n",
    "\n",
    "        self.connection = torch.nn.Linear(d_mnew, self.d_hidden)\n",
    "        self.cout = torch.nn.Linear(self.d_hidden, d_mnew)\n",
    "\n",
    "        self.W_O.weight.data = torch.cat([self.W_O.weight.data, torch.zeros(d_mnew-self.d_model, self.d_model, device=DEVICE)], dim=0)\n",
    "        self.W_O.weight.data = torch.cat([self.W_O.weight.data, torch.zeros(d_mnew, d_mnew-self.d_model,  device=DEVICE)], dim=1)\n",
    "        self.layernorm1 = torch.nn.LayerNorm(d_mnew)\n",
    "        self.layernorm2 = torch.nn.LayerNorm(d_mnew)\n",
    "        for i in range(self.d_model+1, d_mnew):\n",
    "            self.W_O.weight.data[i][i] = 1\n",
    "\n",
    "        for head in self.heads:\n",
    "            head.expand(d_mnew, d_inew)\n",
    "\n",
    "        self.Softmax = torch.nn.LogSoftmax(dim=-1)\n",
    "        self.d_model = d_mnew\n",
    "        self.d_internal = d_inew\n",
    "        self.to(DEVICE)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_blocks, d_model, d_hidden, vocab_size, num_heads, final_dmodel):\n",
    "        super().__init__()\n",
    "        self.num_blocks = num_blocks\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_heads = num_heads\n",
    "        self.SoftMax = torch.nn.LogSoftmax(dim=-1)\n",
    "        self.blocks = torch.nn.ModuleList([Transformer(d_model, num_heads, d_hidden) for _ in range(num_blocks)])\n",
    "        self.d_hidden = d_hidden\n",
    "\n",
    "        self.connection = torch.nn.Linear(d_model, d_hidden)\n",
    "        self.FFN = torch.nn.Sequential(\n",
    "            torch.nn.Dropout(0.1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(d_hidden, d_hidden), \n",
    "            torch.nn.Dropout(0.1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(d_hidden, vocab_size),\n",
    "            torch.nn.LogSoftmax(dim=-1),\n",
    "        )\n",
    "        self.dout = torch.nn.Dropout(0.1)\n",
    "        \n",
    "        self.final_dmodel = final_dmodel\n",
    "        self.embeddings = torch.nn.Embedding(vocab_size, d_model, device=DEVICE)\n",
    "        self.pos_embedding = None\n",
    "        # self.pos_embedding = torch.nn.Embedding(seq_len, d_model, device=DEVICE)\n",
    "        self.generate_pos_embed(d_model)\n",
    "        \n",
    "        self.layernorm = torch.nn.LayerNorm(d_model, device=DEVICE)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x) + self.pos_embedding(torch.arange(x.shape[-1], device=DEVICE))\n",
    "        x = self.dout(x)\n",
    "        t = x\n",
    "        for head in self.blocks:\n",
    "            t = head(t) + t \n",
    "\n",
    "        t = self.layernorm(t)\n",
    "\n",
    "        t = self.connection(t)\n",
    "        ret = self.FFN(t)\n",
    "\n",
    "        return ret\n",
    "\n",
    "    def generate_pos_embed(self, d_model):\n",
    "        # TODO: make more efficient \n",
    "        pos_em = torch.zeros((seq_len, d_model))\n",
    "        for pos in range(seq_len):\n",
    "            for i in range(d_model):\n",
    "                if i % 2 == 0:\n",
    "                    pos_em[pos][i] += torch.sin(torch.tensor(pos/(10000**(2*i/d_model))))\n",
    "                else:\n",
    "                    pos_em[pos][i] += torch.cos(torch.tensor(pos/(10000** (2*i/d_model))))\n",
    "\n",
    "        self.pos_embedding = torch.nn.Embedding.from_pretrained(pos_em, freeze=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def expand(self, d_mnew):\n",
    "        d_inew = d_mnew // self.num_heads\n",
    "        self.connection = torch.nn.Linear(d_mnew, self.d_hidden, device=DEVICE)\n",
    "        self.layernorm = torch.nn.LayerNorm(d_mnew, device=DEVICE)\n",
    "        for block in self.blocks:\n",
    "            block.expand(d_mnew, d_inew)\n",
    "\n",
    "        self.embeddings = torch.nn.Embedding.from_pretrained(torch.cat([self.embeddings.weight, torch.zeros(self.vocab_size, d_mnew-self.d_model, device=DEVICE).uniform_()], dim=1))\n",
    "        # self.pos_embedding = torch.nn.Embedding.from_pretrained(torch.cat([self.pos_embedding.weight, torch.zeros(seq_len, d_mnew-self.d_model, device=DEVICE).uniform_()], dim=1))\n",
    "        self.generate_pos_embed(d_mnew)\n",
    "        # self.embeddings = torch.nn.Embedding(self.vocab_size, d_mnew)\n",
    "        # self.pos_embedding = torch.nn.Embedding(seq_len, d_mnew)\n",
    "\n",
    "        self.d_model = d_mnew\n",
    "        self.d_internal = d_inew\n",
    "        self.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset('Salesforce/wikitext', 'wikitext-103-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test': ['text'], 'train': ['text'], 'validation': ['text']}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data['train']\n",
    "test = data['test']\n",
    "valid = data['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('google-bert/bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, lr=1e-3, min_lr=1e-6):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, max_iters, min_lr)\n",
    "    for iter in range(max_iters):\n",
    "\n",
    "        # every once in a while evaluate the loss on train and val sets\n",
    "        if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "            losses = estimate_loss()\n",
    "            print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}, lr {optimizer.param_groups[0]['lr']}\")\n",
    "\n",
    "        # sample a batch of data\n",
    "        xb, yb = batch('train')\n",
    "\n",
    "        # evaluate the loss\n",
    "        logits = model(xb)\n",
    "        B, T, C = logits.shape\n",
    "        logits = logits.view(B*T, C)\n",
    "        targets = yb.view(B*T)\n",
    "        loss = torch.nn.functional.cross_entropy(logits, targets)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.345857 M parameters\n"
     ]
    }
   ],
   "source": [
    "model = Decoder(num_blocks=4, d_model=128, vocab_size=len(chars), num_heads=4, d_hidden=64*4, final_dmodel=1024)\n",
    "model.to(DEVICE)\n",
    "print(sum(p.numel() for p in model.parameters())/1e6, \"M parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.1688, val loss 4.1690\n",
      "step 100: train loss 2.5251, val loss 2.5221\n",
      "step 200: train loss 2.4296, val loss 2.4312\n",
      "step 300: train loss 2.3301, val loss 2.3314\n",
      "step 400: train loss 2.1980, val loss 2.2196\n",
      "step 500: train loss 2.0995, val loss 2.1247\n",
      "step 600: train loss 2.0298, val loss 2.0811\n",
      "step 700: train loss 1.9629, val loss 2.0257\n",
      "step 800: train loss 1.8915, val loss 1.9733\n",
      "step 900: train loss 1.8468, val loss 1.9321\n",
      "step 1000: train loss 1.8003, val loss 1.8927\n",
      "step 1100: train loss 1.7630, val loss 1.8664\n",
      "step 1200: train loss 1.7296, val loss 1.8358\n",
      "step 1300: train loss 1.6972, val loss 1.8303\n",
      "step 1400: train loss 1.6773, val loss 1.8195\n",
      "step 1500: train loss 1.6501, val loss 1.7725\n",
      "step 1600: train loss 1.6232, val loss 1.7389\n",
      "step 1700: train loss 1.6052, val loss 1.7483\n",
      "step 1800: train loss 1.6014, val loss 1.7330\n",
      "step 1900: train loss 1.5839, val loss 1.7124\n",
      "step 2000: train loss 1.5635, val loss 1.6877\n",
      "step 2100: train loss 1.5527, val loss 1.6970\n",
      "step 2200: train loss 1.5477, val loss 1.6821\n",
      "step 2300: train loss 1.5327, val loss 1.6726\n",
      "step 2400: train loss 1.5224, val loss 1.6568\n",
      "step 2500: train loss 1.5228, val loss 1.6488\n",
      "step 2600: train loss 1.5042, val loss 1.6469\n",
      "step 2700: train loss 1.5006, val loss 1.6230\n",
      "step 2800: train loss 1.4903, val loss 1.6244\n",
      "step 2900: train loss 1.4925, val loss 1.6385\n",
      "step 3000: train loss 1.4818, val loss 1.6191\n",
      "step 3100: train loss 1.4730, val loss 1.6159\n",
      "step 3200: train loss 1.4636, val loss 1.5949\n",
      "step 3300: train loss 1.4665, val loss 1.6082\n",
      "step 3400: train loss 1.4619, val loss 1.5858\n",
      "step 3500: train loss 1.4474, val loss 1.5877\n",
      "step 3600: train loss 1.4466, val loss 1.5897\n",
      "step 3700: train loss 1.4450, val loss 1.5778\n",
      "step 3800: train loss 1.4402, val loss 1.5685\n",
      "step 3900: train loss 1.4319, val loss 1.5781\n",
      "step 4000: train loss 1.4271, val loss 1.5722\n",
      "step 4100: train loss 1.4254, val loss 1.5744\n",
      "step 4200: train loss 1.4213, val loss 1.5673\n",
      "step 4300: train loss 1.4217, val loss 1.5665\n",
      "step 4400: train loss 1.4254, val loss 1.5673\n",
      "step 4500: train loss 1.4038, val loss 1.5610\n",
      "step 4600: train loss 1.4078, val loss 1.5557\n",
      "step 4700: train loss 1.4112, val loss 1.5492\n",
      "step 4800: train loss 1.4048, val loss 1.5446\n",
      "step 4900: train loss 1.3976, val loss 1.5386\n",
      "step 4999: train loss 1.3955, val loss 1.5391\n"
     ]
    }
   ],
   "source": [
    "train(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "# make_dot was moved to https://github.com/szagoruyko/pytorchviz\n",
    "from torchviz import make_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_transfer(model, transfer_step=900, target_size=1024, lr=1e-3, min_lr=1e-6):\n",
    "    loss_func = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, max_iters, min_lr)\n",
    "    for iter in range(1, max_iters):\n",
    "\n",
    "        # every once in a while evaluate the loss on train and val sets\n",
    "        if iter % eval_interval == 0 or iter == max_iters - 1 or iter == 1:\n",
    "            losses = estimate_loss()\n",
    "            print(f\"step {iter}:\\t train loss {losses['train']:.4f}, val loss {losses['val']:.4f}, lr {optimizer.param_groups[0]['lr']}\")\n",
    "\n",
    "        if iter == transfer_step:\n",
    "        # if iter <= 1000 and iter % 500 == 0:\n",
    "            model.expand(target_size)\n",
    "            optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "            # scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 100, 0.5)\n",
    "            print('at step {}: expanded model to: {} M parameters'.format(iter, sum(p.numel() for p in model.parameters())/1e6))\n",
    "            model.to('cpu')\n",
    "            model.to(DEVICE)    # Shortcut to recompile gradient backprop since the model changed sizes\n",
    "            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, max_iters-transfer_step, min_lr)\n",
    "            loss_func = torch.nn.CrossEntropyLoss()\n",
    "        # sample a batch of data\n",
    "        xb, yb = batch('train')\n",
    "\n",
    "        # evaluate the loss\n",
    "        logits = model(xb)\n",
    "        B, T, C = logits.shape\n",
    "        logits = logits.view(B*T, C)\n",
    "        targets = yb.view(B*T)\n",
    "        loss = loss_func(logits, targets)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.535041 M parameters\n"
     ]
    }
   ],
   "source": [
    "model = Decoder(num_blocks=4, d_model=64, vocab_size=len(chars), num_heads=4, d_hidden=64*4, final_dmodel=1024)\n",
    "model.to(DEVICE)\n",
    "print(sum(p.numel() for p in model.parameters())/1e6, \"M parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1: train loss 4.1782, val loss 4.1792\n",
      "step 100: train loss 2.5954, val loss 2.5947\n",
      "step 200: train loss 2.4439, val loss 2.4388\n",
      "step 300: train loss 2.3527, val loss 2.3512\n",
      "step 400: train loss 2.3144, val loss 2.3242\n",
      "step 500: train loss 2.2663, val loss 2.2731\n",
      "step 600: train loss 2.2449, val loss 2.2586\n",
      "step 700: train loss 2.2024, val loss 2.2258\n",
      "step 800: train loss 2.1698, val loss 2.1884\n",
      "step 900: train loss 2.1484, val loss 2.1620\n",
      "at step 900: expanded model to: 0.903617 M parameters\n",
      "step 1000: train loss 2.3213, val loss 2.3277\n",
      "step 1100: train loss 2.2354, val loss 2.2447\n",
      "step 1200: train loss 2.1777, val loss 2.1962\n",
      "step 1300: train loss 2.1376, val loss 2.1698\n",
      "step 1400: train loss 2.1093, val loss 2.1410\n",
      "step 1500: train loss 2.0825, val loss 2.1281\n",
      "step 1600: train loss 2.0704, val loss 2.1064\n",
      "step 1700: train loss 2.0356, val loss 2.0877\n",
      "step 1800: train loss 2.0014, val loss 2.0619\n",
      "step 1900: train loss 1.9868, val loss 2.0340\n",
      "step 2000: train loss 1.9596, val loss 2.0064\n",
      "step 2100: train loss 1.9449, val loss 2.0132\n",
      "step 2200: train loss 1.9344, val loss 1.9988\n",
      "step 2300: train loss 1.9158, val loss 1.9883\n",
      "step 2400: train loss 1.8915, val loss 1.9875\n",
      "step 2500: train loss 1.8823, val loss 1.9621\n",
      "step 2600: train loss 1.8594, val loss 1.9400\n",
      "step 2700: train loss 1.8464, val loss 1.9474\n",
      "step 2800: train loss 1.8400, val loss 1.9392\n",
      "step 2900: train loss 1.8169, val loss 1.9224\n",
      "step 3000: train loss 1.8174, val loss 1.9412\n",
      "step 3100: train loss 1.8030, val loss 1.9187\n",
      "step 3200: train loss 1.7849, val loss 1.9057\n",
      "step 3300: train loss 1.7786, val loss 1.9098\n",
      "step 3400: train loss 1.7671, val loss 1.9031\n",
      "step 3500: train loss 1.7575, val loss 1.8949\n",
      "step 3600: train loss 1.7571, val loss 1.8773\n",
      "step 3700: train loss 1.7395, val loss 1.8621\n",
      "step 3800: train loss 1.7458, val loss 1.8662\n",
      "step 3900: train loss 1.7317, val loss 1.8700\n",
      "step 4000: train loss 1.7301, val loss 1.8733\n",
      "step 4100: train loss 1.7245, val loss 1.8431\n",
      "step 4200: train loss 1.7255, val loss 1.8557\n",
      "step 4300: train loss 1.7153, val loss 1.8501\n",
      "step 4400: train loss 1.7153, val loss 1.8478\n",
      "step 4500: train loss 1.7055, val loss 1.8351\n",
      "step 4600: train loss 1.7017, val loss 1.8344\n",
      "step 4700: train loss 1.6987, val loss 1.8427\n",
      "step 4800: train loss 1.6877, val loss 1.8223\n",
      "step 4900: train loss 1.6785, val loss 1.8032\n",
      "step 4999: train loss 1.6911, val loss 1.8155\n"
     ]
    }
   ],
   "source": [
    "train_transfer(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_transfer_gradual(model, transfer_step=600, final_size=128, start_size=64, final_bus_step=1200,  lr=1e-3):\n",
    "    loss_func = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    # scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1000, 0.5)\n",
    "    step = final_bus_step // transfer_step\n",
    "    step_size = (final_size-start_size)//step\n",
    "    for iter in range(1, max_iters):\n",
    "\n",
    "        # every once in a while evaluate the loss on train and val sets\n",
    "        if iter % eval_interval == 0 or iter == max_iters - 1 or iter == 1:\n",
    "            losses = estimate_loss()\n",
    "            print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "        # if iter <= 1000 and iter % 500 == 0:\n",
    "        if iter % transfer_step == 0 and iter <= final_bus_step:\n",
    "            start_size += step_size\n",
    "            model.expand(start_size)\n",
    "            optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "            # scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 100, 0.5)\n",
    "            print('at step {}: expanded model to: {} M parameters\\tmodel_size: {}'.format(iter, sum(p.numel() for p in model.parameters())/1e6, start_size))\n",
    "            model.to('cpu')\n",
    "            model.to(DEVICE)    # Shortcut to recompile gradient backprop since the model changed sizes\n",
    "\n",
    "            loss_func = torch.nn.CrossEntropyLoss()\n",
    "        # sample a batch of data\n",
    "        xb, yb = batch('train')\n",
    "\n",
    "        # evaluate the loss\n",
    "        logits = model(xb)\n",
    "        B, T, C = logits.shape\n",
    "        logits = logits.view(B*T, C)\n",
    "        targets = yb.view(B*T)\n",
    "        loss = loss_func(logits, targets)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.685761 M parameters\n",
      "step 1: train loss 4.1678, val loss 4.1682\n",
      "step 100: train loss 2.5144, val loss 2.5178\n",
      "step 200: train loss 2.4015, val loss 2.3948\n",
      "step 300: train loss 2.2808, val loss 2.2922\n",
      "at step 300: expanded model to: 1.193365 M parameters\tmodel_size: 204\n",
      "step 400: train loss 2.3386, val loss 2.3418\n",
      "step 500: train loss 2.2482, val loss 2.2624\n",
      "step 600: train loss 2.1824, val loss 2.2040\n",
      "at step 600: expanded model to: 1.885801 M parameters\tmodel_size: 280\n",
      "step 700: train loss 2.3070, val loss 2.3106\n",
      "step 800: train loss 2.2090, val loss 2.2302\n",
      "step 900: train loss 2.1307, val loss 2.1514\n",
      "at step 900: expanded model to: 2.763069 M parameters\tmodel_size: 356\n",
      "step 1000: train loss 2.2723, val loss 2.2772\n",
      "step 1100: train loss 2.1761, val loss 2.1989\n",
      "step 1200: train loss 2.0827, val loss 2.1294\n",
      "at step 1200: expanded model to: 3.825169 M parameters\tmodel_size: 432\n",
      "step 1300: train loss 2.1690, val loss 2.1862\n",
      "step 1400: train loss 2.0768, val loss 2.1126\n",
      "step 1500: train loss 2.0237, val loss 2.0776\n",
      "at step 1500: expanded model to: 5.072101 M parameters\tmodel_size: 508\n",
      "step 1600: train loss 2.1257, val loss 2.1597\n",
      "step 1700: train loss 2.0111, val loss 2.0729\n",
      "step 1800: train loss 1.9792, val loss 2.0520\n",
      "step 1900: train loss 1.9256, val loss 2.0097\n",
      "step 2000: train loss 1.9021, val loss 1.9859\n",
      "step 2100: train loss 1.8725, val loss 1.9630\n",
      "step 2200: train loss 1.8526, val loss 1.9362\n",
      "step 2300: train loss 1.8390, val loss 1.9407\n",
      "step 2400: train loss 1.8046, val loss 1.9060\n",
      "step 2500: train loss 1.7966, val loss 1.9130\n",
      "step 2600: train loss 1.7806, val loss 1.8889\n",
      "step 2700: train loss 1.7660, val loss 1.8783\n",
      "step 2800: train loss 1.7483, val loss 1.8582\n",
      "step 2900: train loss 1.7432, val loss 1.8585\n",
      "step 3000: train loss 1.7291, val loss 1.8318\n",
      "step 3100: train loss 1.7208, val loss 1.8347\n",
      "step 3200: train loss 1.7130, val loss 1.8424\n",
      "step 3300: train loss 1.6962, val loss 1.8287\n",
      "step 3400: train loss 1.7021, val loss 1.8343\n",
      "step 3500: train loss 1.6874, val loss 1.8065\n",
      "step 3600: train loss 1.6861, val loss 1.8072\n",
      "step 3700: train loss 1.6777, val loss 1.8003\n",
      "step 3800: train loss 1.6895, val loss 1.8227\n",
      "step 3900: train loss 1.6722, val loss 1.7863\n",
      "step 4000: train loss 1.6519, val loss 1.7915\n",
      "step 4100: train loss 1.7353, val loss 1.8435\n",
      "step 4200: train loss 1.6589, val loss 1.7974\n",
      "step 4300: train loss 1.6546, val loss 1.7809\n",
      "step 4400: train loss 1.6635, val loss 1.7786\n",
      "step 4500: train loss 1.6590, val loss 1.7708\n",
      "step 4600: train loss 1.6401, val loss 1.7549\n",
      "step 4700: train loss 1.6288, val loss 1.7566\n",
      "step 4800: train loss 1.6291, val loss 1.7467\n",
      "step 4900: train loss 1.6292, val loss 1.7450\n",
      "step 4999: train loss 1.6294, val loss 1.7390\n"
     ]
    }
   ],
   "source": [
    "model = Decoder(num_blocks=4, d_model=128, vocab_size=len(chars), num_heads=4, d_hidden=128, final_dmodel=1024)\n",
    "model.to(DEVICE)\n",
    "print(sum(p.numel() for p in model.parameters())/1e6, \"M parameters\")\n",
    "train_transfer_gradual(model, transfer_step=300, final_bus_step=1500, start_size=128, final_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WORK BENCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.404713 M parameters\n",
      "step 1:\t train loss 4.1814, val loss 4.1809, lr 0.001\n",
      "step 100:\t train loss 2.4459, val loss 2.4551, lr 0.000999033958943103\n",
      "step 200:\t train loss 2.3195, val loss 2.3500, lr 0.0009961005307061032\n",
      "step 300:\t train loss 2.2446, val loss 2.2977, lr 0.0009912111935927526\n",
      "step 400:\t train loss 2.1711, val loss 2.2149, lr 0.0009843852435829092\n",
      "step 500:\t train loss 2.1285, val loss 2.1885, lr 0.0009756496195827828\n",
      "step 600:\t train loss 2.0560, val loss 2.1359, lr 0.0009650387971093778\n",
      "step 700:\t train loss 2.0061, val loss 2.0846, lr 0.0009525946522313713\n",
      "step 800:\t train loss 1.9437, val loss 2.0346, lr 0.0009383662963034076\n",
      "at step 800: expanded model to: 20.643393 M parameters\n",
      "step 900:\t train loss 2.2716, val loss 2.3072, lr 0.0009986032966919998\n",
      "step 1000:\t train loss 2.1260, val loss 2.1910, lr 0.0009944209976994521\n",
      "step 1100:\t train loss 2.0491, val loss 2.1144, lr 0.0009874764921348207\n",
      "step 1200:\t train loss 1.9814, val loss 2.0648, lr 0.0009778086164901761\n",
      "step 1300:\t train loss 1.9194, val loss 2.0100, lr 0.0009654714374477787\n",
      "step 1400:\t train loss 1.8568, val loss 1.9698, lr 0.0009505339495172576\n",
      "step 1500:\t train loss 1.8283, val loss 1.9176, lr 0.0009330796891903266\n",
      "step 1600:\t train loss 1.7917, val loss 1.8976, lr 0.0009132062677708384\n",
      "step 1700:\t train loss 1.7632, val loss 1.8753, lr 0.0008910248254927807\n",
      "step 1800:\t train loss 1.7268, val loss 1.8673, lr 0.0008666594099789983\n",
      "step 1900:\t train loss 1.7107, val loss 1.8281, lr 0.0008402462825165747\n",
      "step 2000:\t train loss 1.6782, val loss 1.7861, lr 0.0008119331560284385\n",
      "step 2100:\t train loss 1.6699, val loss 1.7913, lr 0.0007818783690027804\n",
      "step 2200:\t train loss 1.6477, val loss 1.7667, lr 0.0007502500000000008\n",
      "step 2300:\t train loss 1.6304, val loss 1.7547, lr 0.0007172249276892214\n",
      "step 2400:\t train loss 1.6034, val loss 1.7331, lr 0.0006829878416710155\n",
      "step 2500:\t train loss 1.5921, val loss 1.7085, lr 0.0006477302096182481\n",
      "step 2600:\t train loss 1.5860, val loss 1.7040, lr 0.0006116492065111803\n",
      "step 2700:\t train loss 1.5771, val loss 1.6857, lr 0.0005749466119550002\n",
      "step 2800:\t train loss 1.5539, val loss 1.6913, lr 0.0005378276817464197\n",
      "step 2900:\t train loss 1.5401, val loss 1.6797, lr 0.0005005000000000007\n",
      "step 3000:\t train loss 1.5277, val loss 1.6503, lr 0.0004631723182535817\n",
      "step 3100:\t train loss 1.5224, val loss 1.6457, lr 0.0004260533880450013\n",
      "step 3200:\t train loss 1.5157, val loss 1.6265, lr 0.0003893507934888214\n",
      "step 3300:\t train loss 1.4974, val loss 1.6236, lr 0.0003532697903817537\n",
      "step 3400:\t train loss 1.4929, val loss 1.6195, lr 0.00031801215832898616\n",
      "step 3500:\t train loss 1.4887, val loss 1.6066, lr 0.00028377507231078025\n",
      "step 3600:\t train loss 1.4755, val loss 1.5917, lr 0.00025075000000000086\n",
      "step 3700:\t train loss 1.4723, val loss 1.6018, lr 0.00021912163099722175\n",
      "step 3800:\t train loss 1.4642, val loss 1.5928, lr 0.00018906684397156329\n",
      "step 3900:\t train loss 1.4648, val loss 1.5830, lr 0.00016075371748342648\n",
      "step 4000:\t train loss 1.4458, val loss 1.5799, lr 0.00013434059002100223\n",
      "step 4100:\t train loss 1.4476, val loss 1.5755, lr 0.00010997517450721956\n",
      "step 4200:\t train loss 1.4433, val loss 1.5745, lr 8.779373222916106e-05\n",
      "step 4300:\t train loss 1.4430, val loss 1.5724, lr 6.79203108096732e-05\n",
      "step 4400:\t train loss 1.4408, val loss 1.5713, lr 5.046605048274184e-05\n",
      "step 4500:\t train loss 1.4406, val loss 1.5672, lr 3.552856255222016e-05\n",
      "step 4600:\t train loss 1.4317, val loss 1.5602, lr 2.319138350982274e-05\n",
      "step 4700:\t train loss 1.4308, val loss 1.5621, lr 1.3523507865179137e-05\n",
      "step 4800:\t train loss 1.4363, val loss 1.5627, lr 6.579002300548326e-06\n",
      "step 4900:\t train loss 1.4325, val loss 1.5640, lr 2.396703308000537e-06\n",
      "step 4999:\t train loss 1.4400, val loss 1.5614, lr 1.0001397354639718e-06\n"
     ]
    }
   ],
   "source": [
    "model = Decoder(num_blocks=6, d_model=360, vocab_size=len(chars), num_heads=8, d_hidden=1024, final_dmodel=1024)\n",
    "model.to(DEVICE)\n",
    "print(sum(p.numel() for p in model.parameters())/1e6, \"M parameters\")\n",
    "train_transfer(model, transfer_step=800, target_size=512, lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.643393 M parameters\n",
      "step 0: train loss 4.1703, val loss 4.1703, lr 0.0001\n",
      "step 100: train loss 2.5414, val loss 2.5442, lr 9.991120277927216e-05\n",
      "step 200: train loss 2.4371, val loss 2.4484, lr 9.964516155915146e-05\n",
      "step 300: train loss 2.2989, val loss 2.3068, lr 9.920292628279099e-05\n",
      "step 400: train loss 2.2434, val loss 2.2575, lr 9.858624225078842e-05\n",
      "step 500: train loss 2.1763, val loss 2.1936, lr 9.779754323328188e-05\n",
      "step 600: train loss 2.1150, val loss 2.1282, lr 9.683994186497123e-05\n",
      "step 700: train loss 2.0623, val loss 2.0977, lr 9.571721736097081e-05\n",
      "step 800: train loss 2.0129, val loss 2.0532, lr 9.44338006019738e-05\n",
      "step 900: train loss 1.9783, val loss 2.0294, lr 9.299475664759062e-05\n",
      "step 1000: train loss 1.9239, val loss 1.9899, lr 9.140576474687264e-05\n",
      "step 1100: train loss 1.8907, val loss 1.9573, lr 8.967309592491052e-05\n",
      "step 1200: train loss 1.8533, val loss 1.9423, lr 8.78035882339636e-05\n",
      "step 1300: train loss 1.8167, val loss 1.9096, lr 8.580461976679112e-05\n",
      "step 1400: train loss 1.7966, val loss 1.8989, lr 8.368407953869114e-05\n",
      "step 1500: train loss 1.7715, val loss 1.8761, lr 8.145033635316132e-05\n",
      "step 1600: train loss 1.7545, val loss 1.8767, lr 7.91122057740549e-05\n",
      "step 1700: train loss 1.7236, val loss 1.8376, lr 7.667891533457722e-05\n",
      "step 1800: train loss 1.7118, val loss 1.8355, lr 7.416006812042827e-05\n",
      "step 1900: train loss 1.6876, val loss 1.8143, lr 7.156560487081045e-05\n",
      "step 2000: train loss 1.6742, val loss 1.7965, lr 6.89057647468728e-05\n",
      "step 2100: train loss 1.6594, val loss 1.7949, lr 6.619104492241899e-05\n",
      "step 2200: train loss 1.6498, val loss 1.7740, lr 6.343215915635851e-05\n",
      "step 2300: train loss 1.6348, val loss 1.7682, lr 6.063999551039487e-05\n",
      "step 2400: train loss 1.6216, val loss 1.7634, lr 5.782557337882021e-05\n",
      "step 2500: train loss 1.6095, val loss 1.7473, lr 5.500000000000099e-05\n",
      "step 2600: train loss 1.6026, val loss 1.7400, lr 5.217442662118182e-05\n",
      "step 2700: train loss 1.5907, val loss 1.7247, lr 4.936000448960717e-05\n",
      "step 2800: train loss 1.5831, val loss 1.7276, lr 4.656784084364323e-05\n",
      "step 2900: train loss 1.5729, val loss 1.7183, lr 4.3808955077582306e-05\n",
      "step 3000: train loss 1.5670, val loss 1.7112, lr 4.109423525312808e-05\n",
      "step 3100: train loss 1.5631, val loss 1.6970, lr 3.8434395129190284e-05\n",
      "step 3200: train loss 1.5557, val loss 1.6915, lr 3.5839931879572586e-05\n",
      "step 3300: train loss 1.5539, val loss 1.6959, lr 3.332108466542373e-05\n",
      "step 3400: train loss 1.5377, val loss 1.6858, lr 3.088779422594612e-05\n",
      "step 3500: train loss 1.5332, val loss 1.6834, lr 2.854966364683972e-05\n",
      "step 3600: train loss 1.5382, val loss 1.6757, lr 2.631592046130999e-05\n",
      "step 3700: train loss 1.5254, val loss 1.6754, lr 2.4195380233209977e-05\n",
      "step 3800: train loss 1.5232, val loss 1.6785, lr 2.21964117660373e-05\n",
      "step 3900: train loss 1.5264, val loss 1.6705, lr 2.0326904075090177e-05\n",
      "step 4000: train loss 1.5189, val loss 1.6609, lr 1.8594235253127924e-05\n",
      "step 4100: train loss 1.5180, val loss 1.6558, lr 1.7005243352409754e-05\n",
      "step 4200: train loss 1.5112, val loss 1.6576, lr 1.5566199398026478e-05\n",
      "step 4300: train loss 1.5105, val loss 1.6455, lr 1.4282782639029383e-05\n",
      "step 4400: train loss 1.5124, val loss 1.6483, lr 1.3160058135028885e-05\n",
      "step 4500: train loss 1.5009, val loss 1.6505, lr 1.2202456766718232e-05\n",
      "step 4600: train loss 1.5005, val loss 1.6467, lr 1.1413757749211699e-05\n",
      "step 4700: train loss 1.5069, val loss 1.6513, lr 1.0797073717209061e-05\n",
      "step 4800: train loss 1.5024, val loss 1.6511, lr 1.0354838440848526e-05\n",
      "step 4900: train loss 1.5031, val loss 1.6457, lr 1.0088797220727781e-05\n",
      "step 4999: train loss 1.4942, val loss 1.6451, lr 1.0000008882643672e-05\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = Decoder(num_blocks=6, d_model=512, vocab_size=len(chars), num_heads=8, d_hidden=1024, final_dmodel=1024)\n",
    "model.to(DEVICE)\n",
    "print(sum(p.numel() for p in model.parameters())/1e6, \"M parameters\")\n",
    "train(model, lr=1e-4, min_lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Decoder(\n",
       "  (SoftMax): LogSoftmax(dim=-1)\n",
       "  (blocks): ModuleList(\n",
       "    (0-3): 4 x Transformer(\n",
       "      (heads): ModuleList(\n",
       "        (0-7): 8 x AttentionHead(\n",
       "          (W_Q): Linear(in_features=512, out_features=64, bias=False)\n",
       "          (W_K): Linear(in_features=512, out_features=64, bias=False)\n",
       "          (W_V): Linear(in_features=512, out_features=64, bias=False)\n",
       "          (SoftMax): Softmax(dim=-1)\n",
       "        )\n",
       "      )\n",
       "      (Softmax): LogSoftmax(dim=-1)\n",
       "      (FFN): Sequential(\n",
       "        (0): ReLU()\n",
       "        (1): Dropout(p=0.1, inplace=False)\n",
       "        (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (3): ReLU()\n",
       "        (4): Dropout(p=0.1, inplace=False)\n",
       "        (5): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (6): ReLU()\n",
       "        (7): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (W_O): Linear(in_features=512, out_features=512, bias=False)\n",
       "      (layernorm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (layernorm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (connection): Linear(in_features=512, out_features=1024, bias=True)\n",
       "      (cout): Linear(in_features=1024, out_features=512, bias=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (connection): Linear(in_features=512, out_features=1024, bias=True)\n",
       "  (FFN): Sequential(\n",
       "    (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (1): Dropout(p=0.1, inplace=False)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "    (4): Dropout(p=0.1, inplace=False)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=4096, out_features=65, bias=True)\n",
       "    (7): LogSoftmax(dim=-1)\n",
       "  )\n",
       "  (embeddings): Embedding(65, 512)\n",
       "  (pos_embedding): Embedding(128, 512)\n",
       "  (layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
