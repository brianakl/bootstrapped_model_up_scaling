{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random\n",
    "from torch import optim\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List\n",
    "from utils import *\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler\n",
    "import tqdm\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerLayer(nn.Module):\n",
    "    def __init__(self, d_model, d_internal):\n",
    "        \"\"\"\n",
    "        :param d_model: The dimension of the inputs and outputs of the layer (note that the inputs and outputs\n",
    "        have to be the same size for the residual connection to work)\n",
    "        :param d_internal: The \"internal\" dimension used in the self-attention computation. Your keys and queries\n",
    "        should both be of this length.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        self.W_Q = torch.tensor(np.random.random([d_model, d_internal]), dtype=torch.float64)\n",
    "        self.W_K = torch.tensor(np.random.random([d_model, d_internal]), dtype=torch.float64)\n",
    "        self.W_V = torch.tensor(np.random.random([d_model, d_model]), dtype=torch.float64)\n",
    "        \"\"\"\n",
    "        self.W_Q = torch.nn.Linear(d_model, d_internal, False)\n",
    "        self.W_K = torch.nn.Linear(d_model, d_internal, False)\n",
    "        self.W_V = torch.nn.Linear(d_model, d_model, False)\n",
    "\n",
    "        self.SoftMax = torch.nn.Softmax(dim=-1)\n",
    "\n",
    "        self.FFN = torch.nn.Sequential(\n",
    "            torch.nn.Linear(d_model, d_internal),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(),\n",
    "            torch.nn.Linear(d_internal, d_model)\n",
    "        )\n",
    "        self.d_model = d_model\n",
    "        self.d_internal = d_internal\n",
    "\n",
    "        self.double()\n",
    "\n",
    "\n",
    "        #raise Exception(\"Implement me\")\n",
    "\n",
    "    def forward(self, input_vecs):\n",
    "        \"\"\"\n",
    "        Q = torch.matmul( input_vecs, self.W_Q)\n",
    "        K = torch.matmul(input_vecs, self.W_K)\n",
    "        V = torch.matmul(input_vecs, self.W_V )\n",
    "        \"\"\"\n",
    "        Q = self.W_Q(input_vecs)\n",
    "        K = self.W_K(input_vecs)\n",
    "        V = self.W_V(input_vecs)\n",
    "\n",
    "\n",
    "  \n",
    "        Q = torch.matmul(Q, torch.transpose(K, -2, -1))\n",
    "\n",
    "        Q = Q / torch.sqrt(torch.tensor(self.d_model)) \n",
    "        Attn = self.SoftMax(Q)\n",
    "        a = torch.matmul(Attn, V)\n",
    "\n",
    "        a = a + input_vecs\n",
    "\n",
    "        output = self.FFN(a) + a\n",
    "\n",
    "        \n",
    "        return output, Attn\n",
    "\n",
    "        raise Exception(\"Implement me\")\n",
    "\n",
    "\n",
    "# Implementation of positional encoding that you can use in your network\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, num_positions: int=20, batched=False):\n",
    "        \"\"\"\n",
    "        :param d_model: dimensionality of the embedding layer to your model; since the position encodings are being\n",
    "        added to character encodings, these need to match (and will match the dimension of the subsequent Transformer\n",
    "        layer inputs/outputs)\n",
    "        :param num_positions: the number of positions that need to be encoded; the maximum sequence length this\n",
    "        module will see\n",
    "        :param batched: True if you are using batching, False otherwise\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Dict size\n",
    "        self.emb = nn.Embedding(num_positions, d_model)\n",
    "        self.batched = batched\n",
    "        self.d_model = d_model\n",
    "        self.num_positions = num_positions\n",
    "\n",
    "        self.sinu = torch.zeros((num_positions, d_model)).to(DEVICE)\n",
    "\n",
    "        for pos in range(num_positions):\n",
    "            for m in range(d_model):\n",
    "                if m%2 == 0:\n",
    "                    self.sinu[pos][m] += torch.sin(torch.tensor(pos/(10000**((2*m)/d_model))))\n",
    "                else:\n",
    "                    self.sinu[pos][m] += torch.cos(torch.tensor(pos/(10000**((2*m)/d_model))))\n",
    "\n",
    "        #self.sinu = torch.tensor(self.sinu)\n",
    "\n",
    "    # def change_np(self, np):\n",
    "    #     if np == self.num_positions:\n",
    "    #         return\n",
    "        \n",
    "    #     self.num_positions = np\n",
    "    #     self.sinu = torch.zeros((np, self.d_model))\n",
    "    #     for pos in range(self.num_positions):\n",
    "    #         for m in range(self.d_model):\n",
    "    #             if m%2 == 0:\n",
    "    #                 self.sinu[pos][m] += torch.sin(torch.tensor(pos/(10000**((2*m)/self.d_model))))\n",
    "    #             else:\n",
    "    #                 self.sinu[pos][m] += torch.cos(torch.tensor(pos/(10000**((2*m)/self.d_model))))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: If using batching, should be [batch size, seq len, embedding dim]. Otherwise, [seq len, embedding dim]\n",
    "        :return: a tensor of the same size with positional embeddings added in\n",
    "        \"\"\"\n",
    "\n",
    "        # Second-to-last dimension will always be sequence length\n",
    "        # input_size = x.shape[-2]\n",
    "        # indices_to_embed = torch.tensor(np.asarray(range(0, input_size))).type(torch.LongTensor)\n",
    "        if self.batched:\n",
    "            # Use unsqueeze to form a [1, seq len, embedding dim] tensor -- broadcasting will ensure that this\n",
    "            # gets added correctly across the batch\n",
    "\n",
    "            for b in range(x.shape[0]):\n",
    "                x[b] += self.sinu\n",
    "            return x \n",
    "        else:\n",
    "            return x + self.sinu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: test with various positional embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LetterCountingExample(object):\n",
    "    def __init__(self, input: str, output: np.array, vocab_index: Indexer):\n",
    "        self.input = input\n",
    "        self.input_indexed = np.array([vocab_index.index_of(ci) for ci in input])\n",
    "        self.input_tensor = torch.LongTensor(self.input_indexed)\n",
    "        self.output = output\n",
    "        self.output_tensor = torch.LongTensor(self.output)\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size, num_positions, d_model, d_internal, num_classes, num_layers, **args):\n",
    "        \"\"\"\n",
    "        :param vocab_size: vocabulary size of the embedding layer\n",
    "        :param num_positions: max sequence length that will be fed to the model; should be 20\n",
    "        :param d_model: see TransformerLayer\n",
    "        :param d_internal: see TransformerLayer\n",
    "        :param num_classes: number of classes predicted at the output layer; should be 3\n",
    "        :param num_layers: number of TransformerLayers to use; can be whatever you want\n",
    "        \"\"\" \n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.d_model = d_model\n",
    "        self.d_internal = d_internal\n",
    "        \n",
    "\n",
    "        #self.tformer = []\n",
    "        #for i in range(num_layers):\n",
    "        #    self.tformer.append(TransformerLayer(d_model, d_internal))\n",
    "\n",
    "        self.tformer = TransformerLayer(d_model, d_internal)\n",
    "        #self.tformer1 = TransformerLayer(d_model, d_internal)\n",
    "        self.Softmax = torch.nn.LogSoftmax(dim=-1)\n",
    "        self.FFN = torch.nn.Sequential(\n",
    "            torch.nn.Linear(d_model, d_internal),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(),\n",
    "            torch.nn.Linear(d_internal, num_classes)\n",
    "        )\n",
    "        self.b = False\n",
    "        self.penc = PositionalEncoding(d_model=d_model, num_positions=num_positions, batched=self.b)\n",
    "        self.embed = torch.nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "        self.double()\n",
    "\n",
    "        #raise Exception(\"Implement me\")\n",
    "\n",
    "    def extrap(self, model, method='double'):\n",
    "        if method == 'double':\n",
    "            d_m = self.d_model\n",
    "            d_i = d_m // 2 # = self.d_internal\n",
    "            z = torch.zeros(d_m, d_i, dtype=float)\n",
    "            j = 0\n",
    "\n",
    "            for i in range(0, d_m-1, 2):\n",
    "                z[i][j] = 1\n",
    "                z[i+1][j] = 1\n",
    "                j += 1\n",
    "\n",
    "            z1 = torch.zeros(d_i, d_i//2, dtype=float)\n",
    "            j = 0\n",
    "            for i in range(0, d_i-1, 2):    \n",
    "                z1[i][j] = 1\n",
    "                z1[i+1][j] = 1\n",
    "                j += 1\n",
    "\n",
    "            Q = torch.matmul(z, torch.transpose(model.tformer.W_Q.weight.data, -1, -2))\n",
    "            Q = torch.matmul(Q, torch.transpose(z1, -1,-2))\n",
    "            self.tformer.W_Q.weight.data = torch.transpose(Q, -1, -2)\n",
    "\n",
    "            K = torch.matmul(z, torch.transpose(model.tformer.W_K.weight.data, -1, -2))\n",
    "            K = torch.matmul(z1, torch.transpose(K, -1, -2))\n",
    "            self.tformer.W_K.weight.data = K\n",
    "\n",
    "            V = torch.matmul(z, torch.transpose(model.tformer.W_V.weight.data, -1, -2))\n",
    "            V = torch.matmul(z, torch.transpose(V,-1,-2))\n",
    "            self.tformer.W_V.weight.data = torch.transpose(V, -1, -2)\n",
    "\n",
    "        if method == 'onehot':\n",
    "            # for one hot we simply add the corresponding one hot vector to the dimensions of the vector\n",
    "\n",
    "            Q = model.tformer.W_Q.weight.data\n",
    "            z = torch.zeros((self.d_internal - model.d_internal, model.tformer.d_model)).to(DEVICE)\n",
    "            # z = torch.randn((self.d_internal - model.d_internal, model.tformer.d_model)).to(DEVICE)\n",
    "            Q = torch.cat((Q,z), dim=0)\n",
    "            z = torch.zeros((self.d_model - model.d_model, self.d_model - model.d_model)).to(DEVICE)\n",
    "            # z = torch.randn((self.d_model - model.d_model, self.d_model - model.d_model)).to(DEVICE)\n",
    "            Q = torch.cat((Q,z), dim=-1)\n",
    "            for i in range(self.d_internal):\n",
    "                Q[i][i] = 1 if Q[i][i] == 0 else Q[i][i]\n",
    "            \n",
    "\n",
    "            V = model.tformer.W_V.weight.data\n",
    "            z = torch.zeros((self.d_model - model.d_model, model.tformer.d_model)).to(DEVICE)\n",
    "            # z = torch.randn((self.d_model - model.d_model, model.tformer.d_model)).to(DEVICE)\n",
    "            V = torch.cat((V,z), dim=0)\n",
    "            z = torch.zeros((self.d_model, model.tformer.d_model)).to(DEVICE)\n",
    "            # z = torch.randn((self.d_model, model.tformer.d_model)).to(DEVICE)\n",
    "            V = torch.cat((V,z), dim=-1)\n",
    "            for i in range(self.d_model):\n",
    "                V[i][i] = 1 if V[i][i] == 0 else V[i][i]\n",
    "\n",
    "            K = model.tformer.W_K.weight.data\n",
    "            z = torch.zeros((self.d_internal - model.d_internal, model.tformer.d_model)).to(DEVICE)\n",
    "            # z = torch.randn((self.d_internal - model.d_internal, model.tformer.d_model)).to(DEVICE)\n",
    "            K = torch.cat((K,z), dim=0)\n",
    "            z = torch.zeros((self.tformer.d_internal, model.tformer.d_model)).to(DEVICE)\n",
    "            # z = torch.randn((self.tformer.d_internal, model.tformer.d_model)).to(DEVICE)\n",
    "            K = torch.cat((K,z), dim=-1)\n",
    "            for i in range(self.d_internal):\n",
    "                K[i][i] = 1 if K[i][i] == 0 else K[i][i]\n",
    "\n",
    "\n",
    "            self.tformer.W_Q.weight.data = Q\n",
    "            self.tformer.W_K.weight.data = K\n",
    "            self.tformer.W_V.weight.data = V\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, indices):\n",
    "        \"\"\"\n",
    "\n",
    "        :param indices: list of input indices\n",
    "        :return: A tuple of the softmax log probabilities (should be a 20x3 matrix) and a list of the attention\n",
    "        maps you use in your layers (can be variable length, but each should be a 20x20 matrix)\n",
    "        \"\"\"\n",
    "        t = self.embed(indices)\n",
    "        t = self.penc(t)\n",
    "        t = t.to(torch.float64)\n",
    "        t, attn = self.tformer(t)\n",
    "        #t1, attn = self.tformer1(t) \n",
    "        #t = t1 + t\n",
    "        x = self.FFN(t)\n",
    "        x = self.Softmax(x)\n",
    "        #print(x.shape)\n",
    "        #exit()\n",
    "        return x, [attn]\n",
    "    \n",
    "    def batch(self, b):\n",
    "        self.b = b\n",
    "        self.penc.batched = b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_letter_count_output(input: str, count_only_previous: bool=True) -> np.array:\n",
    "    \"\"\"\n",
    "    :param input: The string\n",
    "    :param count_only_previous: True if we should only count previous occurrences, False for all occurrences\n",
    "    :return: the output for the letter-counting task as a numpy array of 0s, 1s, and 2s\n",
    "    \"\"\"\n",
    "    output = np.zeros(len(input))\n",
    "    for i in range(0, len(input)):\n",
    "        if count_only_previous:\n",
    "            output[i] = min(2, len([c for c in input[0:i] if c == input[i]]))\n",
    "        else:\n",
    "            output[i] = min(2, len([c for c in input if c == input[i]]) - 1)  # count all *other* instances of input[i]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_example(file):\n",
    "    \"\"\"\n",
    "    :param file:\n",
    "    :return: A list of the lines in the file, each exactly 20 characters long\n",
    "    \"\"\"\n",
    "    all_lines = []\n",
    "    for line in open(file):\n",
    "        all_lines.append(line[:-1]) # eat the \\n\n",
    "    \n",
    "    #print(\"%i lines read in\" % len(all_lines))\n",
    "    return all_lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceData(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.n = len(self.data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.data[idx], self.labels[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(model: Transformer, dev_examples: List[LetterCountingExample], do_print=False, do_plot_attn=False):\n",
    "    \"\"\"\n",
    "    Decodes the given dataset, does plotting and printing of examples, and prints the final accuracy.\n",
    "    :param model: your Transformer that returns log probabilities at each position in the input\n",
    "    :param dev_examples: the list of LetterCountingExample\n",
    "    :param do_print: True if you want to print the input/gold/predictions for the examples, false otherwise\n",
    "    :param do_plot_attn: True if you want to write out plots for each example, false otherwise\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    num_correct = 0\n",
    "    num_total = 0\n",
    "    if len(dev_examples) > 100:\n",
    "        if do_print:\n",
    "            print(\"Decoding on a large number of examples (%i); not printing or plotting\" % len(dev_examples))\n",
    "        do_print = False\n",
    "        do_plot_attn = False\n",
    "    for i in range(0, len(dev_examples)):\n",
    "        ex = dev_examples[i]\n",
    "        (log_probs, attn_maps) = model.forward(ex.input_tensor.to(DEVICE))\n",
    "        predictions = np.argmax(log_probs.cpu().detach().numpy(), axis=1)\n",
    "        if do_print:\n",
    "            print(\"INPUT %i: %s\" % (i, ex.input))\n",
    "            print(\"GOLD %i: %s\" % (i, repr(ex.output.astype(dtype=int))))\n",
    "            print(\"PRED %i: %s\" % (i, repr(predictions)))\n",
    "        if do_plot_attn:\n",
    "            for j in range(0, len(attn_maps)):\n",
    "                attn_map = attn_maps[j]\n",
    "                fig, ax = plt.subplots()\n",
    "                im = ax.imshow(attn_map.detach().numpy(), cmap='hot', interpolation='nearest')\n",
    "                ax.set_xticks(np.arange(len(ex.input)), labels=ex.input)\n",
    "                ax.set_yticks(np.arange(len(ex.input)), labels=ex.input)\n",
    "                ax.xaxis.tick_top()\n",
    "                # plt.show()\n",
    "                plt.savefig(\"plots/%i_attns%i.png\" % (i, j))\n",
    "        acc = sum([predictions[i] == ex.output[i] for i in range(0, len(predictions))])\n",
    "        num_correct += acc\n",
    "        num_total += len(predictions)\n",
    "    if do_print:\n",
    "        print(\"Accuracy: %i / %i = %f\" % (num_correct, num_total, float(num_correct) / num_total))\n",
    "    return (num_correct, num_total, float(num_correct) / num_total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(args, train:LetterCountingExample, dev:LetterCountingExample, extrap=None, num_epochs=10):\n",
    "\n",
    "    model = Transformer(**args)\n",
    "    if extrap != None:\n",
    "        model.extrap(extrap, method='onehot')\n",
    "\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    model.train()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "\n",
    "    for i in range(len(train)):\n",
    "        x_train.append(train[i].input_tensor.to(DEVICE))\n",
    "        y_train.append(train[i].output_tensor.to(DEVICE))\n",
    "\n",
    "    ds = SentenceData(x_train, y_train)\n",
    "\n",
    "    data = DataLoader(ds, batch_size=16, shuffle=True)\n",
    "\n",
    "    training_loop(model, data, num_epochs)\n",
    "\n",
    "\n",
    "def training_loop(model, data, dev, num_epochs=10):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "    results = []\n",
    "    avg_loss = []\n",
    "    for t in range(num_epochs):\n",
    "        loss_fnc = nn.NLLLoss()\n",
    "        model.train()\n",
    "        l = 0.\n",
    "        for i, (d, label) in enumerate(data):\n",
    "            py, x = model(d)\n",
    "            loss = loss_fnc(py.view(-1,3), label.view(-1))\n",
    "\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            l += loss.item()\n",
    "             \n",
    "        r = decode(model, dev)\n",
    "\n",
    "        # print(\"epoch {}:\\t\".format(t), decode(model, dev))\n",
    "\n",
    "        avg_loss.append(l/len(data))\n",
    "        model.eval()\n",
    "        results.append(r[-1])\n",
    "    \n",
    "    model.train()\n",
    "    return model, results, avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(args, model=None, train_data='data/lettercounting-train.txt', dev_data='data/lettercounting-dev.txt', do_print=False, do_plot_attn=False, num_epochs=10):\n",
    "    # Constructs the vocabulary: lowercase letters a to z and space\n",
    "    vocab = [chr(ord('a') + i) for i in range(0, 26)] + [' ']\n",
    "    vocab_index = Indexer()\n",
    "    for char in vocab:\n",
    "        vocab_index.add_and_get_index(char)\n",
    "    if do_print:\n",
    "        print(repr(vocab_index))\n",
    "\n",
    "    count_only_previous = True\n",
    "\n",
    "    # Constructs and labels the data\n",
    "    train_exs = read_examples(train_data)\n",
    "    train_bundles = [LetterCountingExample(l, get_letter_count_output(l, count_only_previous), vocab_index) for l in train_exs]\n",
    "    dev_exs = read_examples(dev_data)\n",
    "    dev_bundles = [LetterCountingExample(l, get_letter_count_output(l, count_only_previous), vocab_index) for l in dev_exs]\n",
    "    if model == None:\n",
    "        model = train_classifier(args, train_bundles, dev_bundles, num_epochs=num_epochs)\n",
    "    else:\n",
    "        model = train_classifier(args, train_bundles, dev_bundles, extrap=model, num_epochs=num_epochs)\n",
    "\n",
    "    results = []\n",
    "    # Decodes the first 5 dev examples to display as output\n",
    "    results.append(decode(model, dev_bundles[0:5], do_print=do_print, do_plot_attn=do_plot_attn))\n",
    "    # Decodes 100 training examples and the entire dev set (1000 examples)\n",
    "    if do_print:\n",
    "        print(\"Training accuracy (whole set):\")\n",
    "    results.append(decode(model, train_bundles, do_print))\n",
    "\n",
    "    if do_print:\n",
    "        print(\"Dev accuracy (whole set):\")\n",
    "    results.append(decode(model, dev_bundles, do_print))\n",
    "    \n",
    "    return model, results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_on_axis(arr:List[List]):\n",
    "    n = len(arr)\n",
    "    avg = np.zeros(len(arr[0]))\n",
    "    for row in arr:\n",
    "        avg += np.array(row)\n",
    "    return avg/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(model_args:List):\n",
    "    vocab = [chr(ord('a') + i) for i in range(0, 26)] + [' ']\n",
    "    vocab_index = Indexer()\n",
    "    train_data='data/lettercounting-train.txt' \n",
    "    dev_data='data/lettercounting-dev.txt'\n",
    "    do_print=False\n",
    "    do_plot_attn=False\n",
    "    num_epochs=10\n",
    "    for char in vocab:\n",
    "        vocab_index.add_and_get_index(char)\n",
    "    if do_print:\n",
    "        print(repr(vocab_index))\n",
    "\n",
    "    count_only_previous = True\n",
    "\n",
    "    # Constructs and labels the data\n",
    "    train_exs = read_example(train_data)\n",
    "    train = [LetterCountingExample(l, get_letter_count_output(l, count_only_previous), vocab_index) for l in train_exs]\n",
    "    dev_exs = read_example(dev_data)\n",
    "    dev = [LetterCountingExample(l, get_letter_count_output(l, count_only_previous), vocab_index) for l in dev_exs]   \n",
    "\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "\n",
    "    for i in range(len(train)):\n",
    "        x_train.append(train[i].input_tensor.to(DEVICE))\n",
    "        y_train.append(train[i].output_tensor.to(DEVICE))\n",
    "\n",
    "    ds = SentenceData(x_train, y_train)\n",
    "\n",
    "    data = DataLoader(ds, batch_size=128, shuffle=True)\n",
    "    prev_args = None\n",
    "    num_training_epochs = 10\n",
    "    transfer_ratio = 0.4\n",
    "\n",
    "    axis_numbers = np.array([i for i in range(num_training_epochs)])\n",
    "\n",
    "    for args in model_args:\n",
    "        if prev_args == None:   \n",
    "            prev_args = args\n",
    "            continue\n",
    "        res_std = []\n",
    "        res_tran = []\n",
    "        pprev_args = []\n",
    "        transfer_train = []\n",
    "        full_train = []\n",
    "        transfer_full_train = []\n",
    "        \n",
    "        loss1 = []\n",
    "        loss2 = []\n",
    "        loss3 = []\n",
    "        loss4 = []\n",
    "        \n",
    "        for t in tqdm.tqdm(range(100)):\n",
    "        # for t in range(10):\n",
    "            model = Transformer(**prev_args).to(DEVICE)\n",
    "            model, r1, l1 = training_loop(model, data, dev, num_epochs=int(num_training_epochs*transfer_ratio))\n",
    "            pprev_args.append(r1)\n",
    "            loss1.append(l1)\n",
    "\n",
    "            model_transfer = Transformer(**args).to(DEVICE)\n",
    "            model_transfer.extrap(model, method='onehot')\n",
    "            model_transfer, r2, l2 = training_loop(model_transfer, data, dev, num_epochs=num_training_epochs - int(num_training_epochs*transfer_ratio))\n",
    "            res_tran.append(np.max(r2))\n",
    "            res_tran.append(decode(model_transfer, dev)[-1])\n",
    "            transfer_train.append(r2)\n",
    "            loss2.append(l2)\n",
    "\n",
    "            model_full = Transformer(**args).to(DEVICE)\n",
    "            m, r3, l3 = training_loop(model_full, data, dev, num_epochs=num_training_epochs)\n",
    "            res_std.append(decode(model_full, dev, do_print, do_plot_attn)[-1])\n",
    "            res_tran.append(np.max(r3))\n",
    "            res_tran.append(decode(m, dev)[-1])\n",
    "            full_train.append(r3)\n",
    "            loss3.append(l3)\n",
    "\n",
    "            model_transfer = Transformer(**args).to(DEVICE)\n",
    "            model_transfer.extrap(model, method='onehot')\n",
    "            model_transfer, r4, l4 = training_loop(model_transfer, data, dev, num_epochs=num_training_epochs)\n",
    "            transfer_full_train.append(r4)\n",
    "            loss4.append(l4)\n",
    "\n",
    "        \n",
    "        pprev_args = average_on_axis(pprev_args)\n",
    "        transfer_train = average_on_axis(transfer_train)\n",
    "        full_train = average_on_axis(full_train)\n",
    "        transfer_full_train = average_on_axis(transfer_full_train)\n",
    "\n",
    "        loss1 = average_on_axis(loss1)\n",
    "        loss2 = average_on_axis(loss2)\n",
    "        loss3 = average_on_axis(loss3)\n",
    "        loss4 = average_on_axis(loss4)\n",
    "\n",
    "\n",
    "        prev_args = args\n",
    "\n",
    "        print(\"args: \", args)\n",
    "        \n",
    "        print(\"transfer: \\t {}\\tstd: {}\".format(np.average(res_tran), np.std(res_tran)))\n",
    "        # print(np.max(res_tran))\n",
    "        print(\"full train: \\t{}\\tstd: {}\".format(np.average(res_std), np.std(res_std)))\n",
    "        # print(np.max(res_std))\n",
    "        t_stat, p_val = ttest_ind(res_tran, res_std)\n",
    "        print(\"p-value:\", p_val)\n",
    "\n",
    "        if p_val < 0.05:\n",
    "            print(\"Statistically significant difference between models (p-value =\", p_val, \")\")\n",
    "        else:\n",
    "            print(\"No statistically significant difference between models (p-value =\", p_val, \")\")\n",
    "        # Calculate the effect size\n",
    "        effect_size = np.mean(res_tran) - np.mean(res_std)\n",
    "        print(\"Effect size:\", effect_size)\n",
    "\n",
    "\n",
    "\n",
    "        # plotting accuracy across training epochs\n",
    "        fig, ax = plt.subplots()\n",
    "        \n",
    "        ax.plot(axis_numbers, full_train, label=\"Full training\")\n",
    "        ax.plot(axis_numbers, np.concatenate((pprev_args, transfer_train)), label='small model transfer')\n",
    "        ax.plot(axis_numbers, transfer_full_train, label='transfer full train')\n",
    "        ax.legend()\n",
    "        ax.set_title(\"Learning Rate Comparison (model size {})\".format(args['d_model']))\n",
    "        plt.ylabel(\"Dev set accuracy\")\n",
    "        plt.xlabel(\"Training Epochs\")\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "        plt.savefig(\"images/acc_model_{}.png\".format(args['d_model']))\n",
    "        \n",
    "        # LOSS\n",
    "        fig, ax = plt.subplots()\n",
    "        \n",
    "        ax.plot(axis_numbers, loss3, label=\"Full training\")\n",
    "        ax.plot(axis_numbers, np.concatenate((loss1, loss2)), label='small model transfer')\n",
    "        ax.plot(axis_numbers, loss4, label='transfer full train')\n",
    "        ax.legend()\n",
    "        ax.set_title(\"Loss Rate Comparison (model size {})\".format(args['d_model']))\n",
    "        plt.ylabel(\"Training Loss\")\n",
    "        plt.xlabel(\"Training Epochs\")\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "        plt.savefig(\"images/loss_model_{}.png\".format(args['d_model']))\n",
    "        # print(args)\n",
    "        # print(results)\n",
    "        print() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_pca(models:List):\n",
    "    Qs = []\n",
    "    Ks = []\n",
    "    Vs = []\n",
    "    for model in models:\n",
    "        Qs.append(model.tformer.W_Q.weight.data.detach().numpy())\n",
    "        Ks.append(model.tformer.W_K.weight.data.detach())\n",
    "        Vs.append(model.tformer.W_V.weight.data.detach())\n",
    "        # self.tformer.W_Q.weight.data = torch.transpose(Q, -1, -2)\n",
    "    \n",
    "    n = np.array(Qs).shape\n",
    "    print(np.array(Qs).shape)\n",
    "\n",
    "    pca = PCA(n_components=5, svd_solver='full')\n",
    "    pca.fit(Qs[-1])\n",
    "    print(pca.explained_variance_)\n",
    "    print(pca.singular_values_)\n",
    "    U, S, Vh = LA.svd(Qs[-1])\n",
    "    # print(U)\n",
    "    print(S[:12])\n",
    "    # print(Vh)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eigen_comparison(model_args:List):\n",
    "    # compare the pca of a school of models of the same size?\n",
    "\n",
    "    models = []   \n",
    "    for args in model_args:\n",
    "        models.append([])\n",
    "        for _ in range(1):\n",
    "            m, r = test(args=args)\n",
    "            models[-1].append(m)\n",
    "            print(r)\n",
    "        do_pca(models[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Device used:  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|â–Ž         | 3/100 [01:58<1:03:36, 39.35s/it]"
     ]
    }
   ],
   "source": [
    "model_args = [\n",
    "    # {'vocab_size':27, 'num_positions':20, 'd_model':24, 'd_internal':12, 'num_classes':3, 'num_layers':1},    # these two model sizes are too small to transfer information\n",
    "    # {'vocab_size':27, 'num_positions':20, 'd_model':48, 'd_internal':24, 'num_classes':3, 'num_layers':1},\n",
    "    {'vocab_size':27, 'num_positions':20, 'd_model':96, 'd_internal':48, 'num_classes':3, 'num_layers':1},\n",
    "    {'vocab_size':27, 'num_positions':20, 'd_model':192, 'd_internal':96, 'num_classes':3, 'num_layers':1},\n",
    "    {'vocab_size':27, 'num_positions':20, 'd_model':384, 'd_internal':192, 'num_classes':3, 'num_layers':1},\n",
    "    {'vocab_size':27, 'num_positions':20, 'd_model':768, 'd_internal':384, 'num_classes':3, 'num_layers':1},\n",
    "]\n",
    "print(\"CUDA Device used: \", DEVICE)\n",
    "compare(model_args)\n",
    "# eigen_comparison(model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Transformer.__init__() got an unexpected keyword argument 'num_epochs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Changing num epochs\u001b[39;00m\n\u001b[1;32m      2\u001b[0m args \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvocab_size\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m27\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_positions\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m20\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124md_model\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m24\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124md_internal\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m12\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_classes\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m3\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_layers\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_epochs\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m20\u001b[39m}\n\u001b[0;32m----> 4\u001b[0m model, results \u001b[38;5;241m=\u001b[39m \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(results)\n",
      "Cell \u001b[0;32mIn[9], line 18\u001b[0m, in \u001b[0;36mtest\u001b[0;34m(args, model, train_data, dev_data, do_print, do_plot_attn, num_epochs)\u001b[0m\n\u001b[1;32m     16\u001b[0m dev_bundles \u001b[38;5;241m=\u001b[39m [LetterCountingExample(l, get_letter_count_output(l, count_only_previous), vocab_index) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m dev_exs]\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 18\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_classifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_bundles\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdev_bundles\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     20\u001b[0m     model \u001b[38;5;241m=\u001b[39m train_classifier(args, train_bundles, dev_bundles, extrap\u001b[38;5;241m=\u001b[39mmodel, num_epochs\u001b[38;5;241m=\u001b[39mnum_epochs)\n",
      "Cell \u001b[0;32mIn[7], line 5\u001b[0m, in \u001b[0;36mtrain_classifier\u001b[0;34m(args, train, dev, extrap, num_epochs)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_classifier\u001b[39m(args,  train: LetterCountingExample, dev: LetterCountingExample, extrap\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# The following code DOES NOT WORK but can be a starting point for your implementation\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# Some suggested snippets to use:\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m extrap \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m      7\u001b[0m         model\u001b[38;5;241m.\u001b[39mextrap(extrap)\n",
      "\u001b[0;31mTypeError\u001b[0m: Transformer.__init__() got an unexpected keyword argument 'num_epochs'"
     ]
    }
   ],
   "source": [
    "#Changing num epochs\n",
    "args = {'vocab_size':27, 'num_positions':20, 'd_model':24, 'd_internal':12, 'num_classes':3, 'num_layers':1, 'num_epochs': 20}\n",
    "\n",
    "model, results = test(args)\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
